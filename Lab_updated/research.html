<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <title>DaSH Lab - Research</title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="description" content="DaSH Lab, Research, BITS Pilani, K K Birla Goa Campus, India">
      <meta name="author" content="">
      <!-- Le styles -->
      <link href="css/bootstrap.min.css" rel="stylesheet">
      <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
      <link href="css/theme.css" rel="stylesheet">
   </head>
   <body>
      <div class="container">
         <header class="jumbotron subhead" id="overview">
            <p class="lead"> BITS Pilani, K. K. Birla Goa Campus, India </p>
            <h1>DaSH Lab (Data, Systems and HPC)</h1>
         </header>
         <div class="masthead">
            <div class="navbar">
               <div class="navbar-inner">
                  <div class="container">
                     <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                      </button>
                      <a class="brand d-none-md-full" href="#" style="margin-left: 3px;">Navigation</a>
                      <div class="nav-collapse collapse" style="height: 0; overflow: hidden;">
                        <ul class="nav">
                           <li><a href="index.html">Home</a></li>
                           <li><a href="people.html">People</a></li>
                           <li class="active"><a href="#">Research</a></li>
                           <li><a href="publications.html">Publications</a></li>
                           <li><a href="gallery.html">Gallery</a></li>
                           <li><a href="collaborations.html">Partners</a></li>
                           <li><a href="equipment.html">Equipment</a></li>
                           <li><a href="joinus.html">Join Us</a></li>
                           <li><a href="contact.html">Contact</a></li>
                        </ul>
                     </div>
                  </div>
               </div>
            </div>
         </div>
         <div class="row-fluid">
            <div class="span3" id="navparent">
               <ul class="nav nav-list bs-docs-sidenav" data-spy="affix" data-offset-top="260" style="width: fit-content; max-height: 30em; overflow: scroll;"> 
                  <!---<li><a href="#statemet"> Lorem Ipsum </a></li>--->
                  <li><a href="#hpc"> High Performance Computing (HPC) </a></li>
                  <li><a class="subhead" href="#pfs"> Parallel File System </a></li>                  
                  <li><a class="subhead" href="#datacache"> Data Caching </a></li>
                  <li><a class="subhead" href="#resourcesched"> Resource Scheduling  </a></li>
                  <li><a class="subhead" href="#ornl"> Supercomputer Log Analysis </a></li>
                  <li><a href="#distributed">Distributed Systems</a></li>
                  <li><a class="subhead" href="#serverless"> Trust in Serverless </a></li>
                  <li><a class="subhead" href="#iac"> Infrastructure as Code </a></li>
                  <li><a class="subhead" href="#smartcontract"> Blockchain Smart Contracts </a></li>
                  <li><a class="subhead" href="#containerdedup"> Container deduplication </a></li>
                  <li><a href="#sysml">Systems for ML</a></li>
                  <li><a class="subhead" href="#bcfl"> Trust in Fed Learning (FL) </a></li>
                  <li><a class="subhead" href="#mlops"> MLOps </a></li>
                  <li><a class="subhead" href="#smartcontract"> Data Caching in TinyML </a></li>
                  <li><a class="subhead" href="#smartcontract"> Heterogeneity Aware FL </a></li>
                  <li><a href="#data">Data</a></li>
                  <li><a class="subhead" href="#bcfl"> Data Compression </a></li>
                  <li><a class="subhead" href="#dataspaces"> Data Spaces</a></li>
                  <li><a class="subhead" href="#mlops"> IO Uring </a></li>
                  <li><a class="subhead" href="#smartcontract"> I/O in Fog Compute </a></li>
               </ul>
            </div>
            <div class="span8 offset1">
               <!---<section id="statemet">
                  <div class="page-header">
                     <h3>Lorem Ipsum</h3>
                     <hr>
                     <p>At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas 
                        molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et 
                        dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil 
                        impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam 
                        et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum 
                        rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.
                     </p>
                  </div>
               </section>--->
               <section id="hpc">
                  <div class="page-header">
                     <h3>High Performance Computing (HPC)</h3>
                     <p>DaSH Lab works on the data and file system aspect of HPC. Most emerging HPC applications have complicated I/O patterns which result in sub-optimal application performance.</p>
                  </div>
                  <div class="row-fluid">
                     <div class="span12">
                        <section id="pfs">
                           <h4>Parallel File System</h4>
                           <br/>
                           <img src="images/pfs.jpg" class="float-right" style="height: 5cm;" alt="pfs"/>
                           <p>
                              <ul>
                                 <li>How do you load balance I/O on to storage servers in a seamless manner?</li>
                                 <li>What is an optimal striping layout for a file within an application?</li>
                                 <li>Where to place files in a heterogeneous storage setup?</li>
                                 <li>How does network and I/O interact with each other in a parallel file system?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Virginia Tech - USA, Johannes Gutenberg University Mainz - Germany
                              <br/>
                              <b>BITS Pilani Personnel: </b>Joel Tony, Yash Bhisikar, Bhavya Bajaj, Kaaviya Uthirapandian, Arnav Borkar, Hari Vamsi, Tushar Barman, Sreenath M.
                              <br/>
                              <b>Other Personnel: </b>Debasmita Biswas, <a href="https://researchprofiles.uni-mainz.de/19180-sarah-neuwirth">Sarah Neuwirth</a>
                              <br/>
                              <b>PFS investigated: </b>Lustre, IBM Spectrum Scale, BeeGFS, Ceph
                              <br/>
                              <b>Few Published Papers: </b><a href="https://dl.acm.org/doi/abs/10.1145/3641885">ACM TOS 2024</a>, <a href="https://doi.org/10.1109/CLUSTERWorkshops61457.2023.00010">Cluster 2023 (1)</a>, <a href="https://doi.org/10.1109/CLUSTERWorkshops61457.2023.00014">Cluster 2023 (2)</a>, <a href="https://doi.org/10.1109/INDIS54524.2021.00011">INDIS@SC21</a>, <a href="https://doi.org/10.1109/CLUSTER.2019.8891045">Cluster19</a>, <a href="https://doi.org/10.1109/IPDPS.2019.00070">IPDPS19</a>
                           </p>
                        </section>
                        <hr>
                        <section id="datacache">
                           <h4>Data Caching</h4>
                           <br/>
                           <img src="images/hvac.jpg" class="float-right" style="height: 7cm;" alt="Quam Id Leo"/>
                           <p>
                              <ul>
                                 <li>How to remove I/O bottleneck for large-scale deep learning applications?</li>
                                 <li>How can we use importance sampling for Distributed DL applications?</li>
                                 <li>Can I/O bottleneck be reduced for HPC applications by data prefetching?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Oak Ridge National Laboratory - USA, University of Viginia - USA, Virginia Tech - USA, Georgia State University
                              <br/>
                              <b>Other Personnel: </b><a href="https://www.olcf.ornl.gov/directory/staff-member/ahmad-maroof-karimi/">Ahmad Maroof Karimi</a> , <a href="https://www.linkedin.com/in/redwankhan/"> Redwan Khan</a>, <a href="https://tddg.github.io/">Yue Cheng</a>, <a href="https://www.ornl.gov/staff-profile/jong-youl-choi">Jong Choi</a>, <a href="https://lwan86.github.io/"> Lipeng Wan</a>
                              <br/>
                              <b>Few Published Papers: </b><a href="https://www.usenix.org/conference/fast23/presentation/khan">FAST23</a>, <a href="https://doi.org/10.1109/CLUSTER51413.2022.00044">Cluster22</a>
                           </p>
                        </section>
                        <hr>
                        <section id="resourcesched">
                           <h4>Resource Scheduling</h4>
                           <br/>
                           <img src="images/sched.jpg" class="float-right" style="height: 3cm;" alt="sched"/>
                           <p>
                              <ul>
                                 <li>How can distributed deep learning jobs be made GPU heterogenity aware?</li>
                                 <li>How can distributed inferencing jobs be made GPU Heterogeneity aware?</li>
                                 <li>How can containers be used in HPC environments?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Virginia Tech - USA, Purdue University - USA, IIT Indore, Kuwait University, Northwestern Polytechnical University - China
                              <br/>
                              <b>BITS Pilani Personnel: </b>Aditya Shiva Sharma, Kinshuk Goel, Amey Patil, Mukul Krishnan
                              <br/>
                              <b>Other Personnel: </b><a href="https://davisjam.github.io/"> James Davis</a>, <a href="http://people.iiti.ac.in/~sidharth/"> Siddharth Sharma</a>, <a href="https://hadeelalbahar.github.io/"> Hadeel Albahar</a>, <a href="https://www.linkedin.com/in/nannan-zhao-882617126/"> Nannan Zhao</a>
                              <br/>
                              <b>Few Published Papers: </b><a href="https://doi.org/10.1109/CCGrid54584.2022.00079">CCGrid22</a>, <a href="https://doi.org/10.1109/CLOUD49709.2020.00048">Cloud20</a>
                           </p>
                           </section>
                           <hr>
                           <section id="ornl">
                              <h4>Supercomputer Log Analysis</h4>
                              <br/>
                              <img src="images/darshan.jpg" class="float-right" style="height: 3cm;" alt="sched"/>
                              <p>
                                 <ul>
                                    <li>How can darshan I/O characterization logs be used to characterize ML workloads?</li>
                                    <li>How can darshan logs be used for HPC trace generation?</li>
                                    <li>How are different supercomputing stacks used by varying HPC workloads?</li>
                                    <li>How do DL workloads differ in academia, industry and national labortories?</li>
                                 </ul>
                              </p>
                              <p>
                                 <b>Collaborators: </b>Virginia Tech - USA, Oak Ridge National Laboratory, Argonne National Laboratory, Lawrence Berkeley Laboratory
                                 <br/>
                                 <b>BITS Pilani Personnel: </b>Natasha Meena Joseph, S Sai Vineet, Kunal Korgaonkar, Snehanshu Saha
                                 <br/>
                                 <b>Other Personnel: </b><a href="https://www.linkedin.com/in/ahmad-hossein-yazdani-6560b3138/"> Ahmad Yazdani</a>,<a href="https://www.olcf.ornl.gov/directory/staff-member/ahmad-maroof-karimi/">Ahmad Maroof Karimi</a>, Jong Choi, Phil Carns, Bing Xie, Suren Byna, Feiyi Wang, Jean Luca Bez
                                 <br/>
                                 <b>Few Published Papers: </b><a href="https://doi.org/10.1145/3571306.3571414">ICDCN23</a>, <a href="https://doi.org/10.1109/MASCOTS53633.2021.9614303">MASCOTS21</a>, <a href="https://doi.org/10.1145/3502181.3531461">HPDC22</a>, <a href="https://doi.org/10.1145/3502181.3531457">HPDC22</a>
                              </p>
                           </section>
                           <hr>
                     </div>
                  </div>
               </section>
               <hr>
               <section id="distributed">
                  <div class="page-header">
                     <h3>Distributed Systems</h3>
                     <p>DaSH Lab works on emerging areas of distributed systems, such as, serverless compute, blockchain, cloud computing, infrastructure-as-code. All projects use state-of-the-art technologies like Kafka, Ethereum, Kubernetes, Docker.</p>
                  </div>
                  <div class="row-fluid">
                     <div class="span12">
                        <section id="serverless">
                           <h4>Trust in Serverless</h4>
                           <br/>
                           <img src="images/serverless.jpg" class="float-right" style="height: 5cm;" alt="pfs"/>
                           <p>
                              <ul>
                                 <li>How do you load balance I/O on to storage servers in a seamless manner?</li>
                                 <li>What is an optimal striping layout for a file within an application?</li>
                                 <li>Where to place files in a heterogeneous storage setup?</li>
                                 <li>How does network and I/O interact with each other in a parallel file system?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Virginia Tech - USA, Johannes Gutenberg University Mainz - Germany
                              <br/>
                              <b>BITS Pilani Personnel: </b>Arnav Borkar, Joel Tony, Yash Bhisikar, Arkaprava Roy, Tushar Barman, Hari Vamsi, Sreenath T M
                              <br/>
                              <b>Other Personnel: </b>Debasmita Biswas, Sarah Neuwirth
                              <br/>
                              <b>PFS investigated: </b>Lustre, IBM Spectrum Scale, BeeGFS, Ceph
                              <br/>
                              <b>Few Published Papers: </b><a href="https://doi.org/10.1109/INDIS54524.2021.00011">INDIS@SC21</a>, <a href="https://doi.org/10.1109/CLUSTER.2019.8891045">Cluster19</a>, <a href="https://doi.org/10.1109/IPDPS.2019.00070">IPDPS19</a>
                           </p>
                        </section>
                        <hr>
                        <section id="mlops">
                           <h4>Data Caching</h4>
                           <br/>
                           <img src="images/hvac.jpg" class="float-right" style="height: 7cm;" alt="Quam Id Leo"/>
                           <p>
                              <ul>
                                 <li>How to remove I/O bottleneck for large-scale deep learning applications?</li>
                                 <li>How can we use importance sampling for Distributed DL applications?</li>
                                 <li>Can I/O bottleneck be reduced for HPC applications by data prefetching?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Oak Ridge National Laboratory - USA, University of Viginia - USA, Virginia Tech - USA, Georgia State University
                              <br/>
                              <b>Other Personnel: </b>Ahmad Maroof Karimi, Redwan Khan, Yue Cheng, Jong Choi, Lipeng Wan
                              <br/>
                              <b>Few Published Papers: </b><a href="https://doi.org/10.1109/CLUSTER51413.2022.00044">Cluster22</a>, <a href="https://www.usenix.org/conference/fast23/presentation/khan">FAST23</a>
                           </p>
                        </section>
                        <hr>
                        <section id="resourcesched">
                           <h4>Resource Scheduling</h4>
                           <br/>
                           <img src="images/sched.jpg" class="float-right" style="height: 3cm;" alt="sched"/>
                           <p>
                              <ul>
                                 <li>How can distributed deep learning jobs be made GPU heterogenity aware?</li>
                                 <li>How can distributed inferencing jobs be made GPU Heterogeneity aware?</li>
                                 <li>How can containers be used in HPC environments?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Virginia Tech - USA, Purdue University - USA, IIT Indore, Kuwait University, Northwestern Polytechnical University - China
                              <br/>
                              <b>BITS Pilani Personnel: </b>Kinshuk Goel, Amey Patil
                              <br/>
                              <b>Other Personnel: </b>James Davis, Siddharth Sharma, Hadeel Albahar, Nannan Zhao
                              <br/>
                              <b>Few Published Papers: </b><a href="https://doi.org/10.1109/CCGrid54584.2022.00079">CCGrid22</a>, <a href="https://doi.org/10.1109/CLOUD49709.2020.00048">Cloud20</a>
                           </p>
                           </section>
                           <hr>
                           <section id="ornl">
                              <h4>Supercomputer Log Analysis</h4>
                              <br/>
                              <img src="images/darshan.jpg" class="float-right" style="height: 3cm;" alt="sched"/>
                              <p>
                                 <ul>
                                    <li>How can darshan I/O characterization logs be used to characterize ML workloads?</li>
                                    <li>How can darshan logs be used for HPC trace generation?</li>
                                    <li>How are different supercomputing stacks used by varying HPC workloads?</li>
                                    <li>How do DL workloads differ in academia, industry and national labortories?</li>
                                 </ul>
                              </p>
                              <p>
                                 <b>Collaborators: </b>Virginia Tech - USA, Oak Ridge National Laboratory, Argonne National Laboratory, Lawrence Berkeley Laboratory
                                 <br/>
                                 <b>BITS Pilani Personnel: </b>Natasha Meena Joseph, S Sai Vineet, Kunal Korgaonkar, Snehanshu Saha
                                 <br/>
                                 <b>Other Personnel: </b>Ahmad Yazdani, Ahmad Maroof Karimi, Jong Choi, Phil Carns, Bing Xie, Suren Byna, Feiyi Wang, Jean Luca Bez
                                 <br/>
                                 <b>Few Published Papers: </b><a href="https://doi.org/10.1145/3571306.3571414">ICDCN23</a>, <a href="https://doi.org/10.1109/MASCOTS53633.2021.9614303">MASCOTS21</a>, <a href="https://doi.org/10.1145/3502181.3531461">HPDC22</a>, <a href="https://doi.org/10.1145/3502181.3531457">HPDC22</a>
                              </p>
                           </section>
                           <hr>
                     </div>
                  </div>
               </section>
               <hr>
               <section id="hpc">
                  <div class="page-header">
                     <h3>High Performance Computing (HPC)</h3>
                     <p>DaSH Lab works on the data and file system aspect of HPC. Most emerging HPC applications have complicated I/O patterns which result in sub-optimal application performance.</p>
                  </div>
                  <div class="row-fluid">
                     <div class="span12">
                        <section id="pfs">
                           <h4>Parallel File System</h4>
                           <br/>
                           <img src="images/pfs.jpg" class="float-right" style="height: 5cm;" alt="pfs"/>
                           <p>
                              <ul>
                                 <li>How do you load balance I/O on to storage servers in a seamless manner?</li>
                                 <li>What is an optimal striping layout for a file within an application?</li>
                                 <li>Where to place files in a heterogeneous storage setup?</li>
                                 <li>How does network and I/O interact with each other in a parallel file system?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Virginia Tech - USA, Johannes Gutenberg University Mainz - Germany
                              <br/>
                              <b>BITS Pilani Personnel: </b>Arnav Borkar, Joel Tony, Yash Bhisikar, Arkaprava Roy, Tushar Barman, Hari Vamsi, Sreenath T M
                              <br/>
                              <b>Other Personnel: </b>Debasmita Biswas, Sarah Neuwirth
                              <br/>
                              <b>PFS investigated: </b>Lustre, IBM Spectrum Scale, BeeGFS, Ceph
                              <br/>
                              <b>Few Published Papers: </b><a href="https://doi.org/10.1109/INDIS54524.2021.00011">INDIS@SC21</a>, <a href="https://doi.org/10.1109/CLUSTER.2019.8891045">Cluster19</a>, <a href="https://doi.org/10.1109/IPDPS.2019.00070">IPDPS19</a>
                           </p>
                        </section>
                        <hr>
                        <section id="datacache">
                           <h4>Data Caching</h4>
                           <br/>
                           <img src="images/hvac.jpg" class="float-right" style="height: 7cm;" alt="Quam Id Leo"/>
                           <p>
                              <ul>
                                 <li>How to remove I/O bottleneck for large-scale deep learning applications?</li>
                                 <li>How can we use importance sampling for Distributed DL applications?</li>
                                 <li>Can I/O bottleneck be reduced for HPC applications by data prefetching?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Oak Ridge National Laboratory - USA, University of Viginia - USA, Virginia Tech - USA, Georgia State University
                              <br/>
                              <b>Other Personnel: </b>Ahmad Maroof Karimi, Redwan Khan, Yue Cheng, Jong Choi, Lipeng Wan
                              <br/>
                              <b>Few Published Papers: </b><a href="https://doi.org/10.1109/CLUSTER51413.2022.00044">Cluster22</a>, <a href="https://www.usenix.org/conference/fast23/presentation/khan">FAST23</a>
                           </p>
                        </section>
                        <hr>
                        <section id="resourcesched">
                           <h4>Resource Scheduling</h4>
                           <br/>
                           <img src="images/sched.jpg" class="float-right" style="height: 3cm;" alt="sched"/>
                           <p>
                              <ul>
                                 <li>How can distributed deep learning jobs be made GPU heterogenity aware?</li>
                                 <li>How can distributed inferencing jobs be made GPU Heterogeneity aware?</li>
                                 <li>How can containers be used in HPC environments?</li>
                              </ul>
                           </p>
                           <p>
                              <b>Collaborators: </b>Virginia Tech - USA, Purdue University - USA, IIT Indore, Kuwait University, Northwestern Polytechnical University - China
                              <br/>
                              <b>BITS Pilani Personnel: </b>Kinshuk Goel, Amey Patil
                              <br/>
                              <b>Other Personnel: </b>James Davis, Siddharth Sharma, Hadeel Albahar, Nannan Zhao
                              <br/>
                              <b>Few Published Papers: </b><a href="https://doi.org/10.1109/CCGrid54584.2022.00079">CCGrid22</a>, <a href="https://doi.org/10.1109/CLOUD49709.2020.00048">Cloud20</a>
                           </p>
                           </section>
                           <hr>
                           <section id="ornl">
                              <h4>Supercomputer Log Analysis</h4>
                              <br/>
                              <img src="images/darshan.jpg" class="float-right" style="height: 3cm;" alt="sched"/>
                              <p>
                                 <ul>
                                    <li>How can darshan I/O characterization logs be used to characterize ML workloads?</li>
                                    <li>How can darshan logs be used for HPC trace generation?</li>
                                    <li>How are different supercomputing stacks used by varying HPC workloads?</li>
                                    <li>How do DL workloads differ in academia, industry and national labortories?</li>
                                 </ul>
                              </p>
                              <p>
                                 <b>Collaborators: </b>Virginia Tech - USA, Oak Ridge National Laboratory, Argonne National Laboratory, Lawrence Berkeley Laboratory
                                 <br/>
                                 <b>BITS Pilani Personnel: </b>Natasha Meena Joseph, S Sai Vineet, Kunal Korgaonkar, Snehanshu Saha
                                 <br/>
                                 <b>Other Personnel: </b>Ahmad Yazdani, Ahmad Maroof Karimi, Jong Choi, Phil Carns, Bing Xie, Suren Byna, Feiyi Wang, Jean Luca Bez
                                 <br/>
                                 <b>Few Published Papers: </b><a href="https://doi.org/10.1145/3571306.3571414">ICDCN23</a>, <a href="https://doi.org/10.1109/MASCOTS53633.2021.9614303">MASCOTS21</a>, <a href="https://doi.org/10.1145/3502181.3531461">HPDC22</a>, <a href="https://doi.org/10.1145/3502181.3531457">HPDC22</a>
                              </p>
                           </section>
                           <hr>
                     </div>
                  </div>
               </section>
               <hr>
            </div>
         </div>
      </div>
      <footer id="footer">
         <div class="container-fluid">
            <div class="row-fluid">
               <div class="span5">
                  <h3>Contact Information</h3>
                     <h5 style="color:white;"><b>Lab Meeting: </b>Tuesdays (6:30 PM - 7:30 PM IST)</h5>
                     <h5 style="color:white;"><b>PI: </b> <a href = "https://arnabkrpaul.github.io/">Arnab K. Paul</a></h5> 
                     <h5 style="color:white;"><b>Email 1: </b>dashlab [DOT] cs [AT] gmail [DOT] com</h5>                    
                     <h5 style="color:white;"><b>Email 2: </b>arnabp [AT] goa [DOT] bits-pilani [DOT] ac [DOT] in</h5>
                     <!---<p><b>Cell: </b>999-999-9999</p>
                     <a href="mailto:your.email@uni.edu">Email</a>--->
               </div>
               <div class="span2">
                  <a href="index.html"><img src = "images/FullLogo.png" alt="research-lab-logo"/></a>
               </div>
               <div class="span5">
                  <h3 class="">Lab Address</h3>
                  <h5 style="color:white">D-241, Department of CSIS<br>
                     BITS Pilani, K K Birla Goa Campus<br>
                     Zuarinagar, Goa<br>
                     India - 403726
                  </h5>
               </div>
                  <!-- <a href="http://maps.google.com/">Show Map</a> -->
               </div>
            </div>
         </div>
      </footer>

      <!-- Le javascript
         ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="js/jquery-1.9.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script>
         $(document).ready(function() {
             $(document.body).scrollspy({
                 target: "#navparent"
             });
         });
         
      </script>
   </body>
</html>